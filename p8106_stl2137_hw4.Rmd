---
title: "p8106_stl2137_hw4"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lasso2)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(ranger)
library(caret)
library(ISLR)
```

# Question 1

```{r}
### Loading in prostate data
data("Prostate") 
dat_prostate <- Prostate
```

## Part A

```{r}
set.seed(1)

tree_prost_1 <- rpart(formula = lpsa ~., data = dat_prostate)
rpart.plot(tree_prost_1)

### Finding lowest cp 
cp_table <- printcp(tree_prost_1)
#plotcp(tree_prost_1)

minErr <- which.min(cp_table[,4])

# minimum cross-validation error
tree_prost_3 <- prune(tree_prost_1, cp = cp_table[minErr, 1])

# 1SE rule
tree_prost_4 <- prune(tree_prost_1, cp = 
                        cp_table[cp_table[,4] < cp_table[minErr, 4] + cp_table[minErr, 5], 1][1])

rpart.plot(tree_prost_3)
rpart.plot(tree_prost_4)
```

The tree size that corresponds to the lowest cross-validation error is 8. This is not the same tree size as the one obtained using the 1 SE rule, as the tree size is 3. 

## Part B


```{r}
plotcp(tree_prost_1)
```

Based off the leftmost value for which the mean lies below the horizontal line in the cp plot, a cp = 0.1 and a tree size of 3 should be utilized. 

```{r}
set.seed(1)

final_tree_prost <- rpart(lpsa ~ ., data = dat_prostate,
                          control = rpart.control(cp = 0.1))

rpart.plot(final_tree_prost)
```

If you have a log cancer volume greater than 2.5, we predict that you will have a log prostate specific antigen level of 3.8. 

## Part C

```{r}
set.seed(1)

bagging_prost <- randomForest(lpsa ~ ., data = dat_prostate,
                              mtry = 8)

bagging_prost$importance
```

The variable for log cancer volume, at a value of `r bagging_prost$importance[1]`, has the highest variable importance. The variable for log of prostate weight, at the value of `r bagging_prost$importance[2]`, has the 2nd highest variable importance.

### Using caret for RF
```{r}
control <- trainControl(method = "cv")

rf_grid_prost <- expand.grid(mtry = 1:8,
                       splitrule = "variance",
                       min.node.size = 1:8)
set.seed(1)
rf_fit_prost <- train(lpsa ~., dat_prostate, 
                method = "ranger",
                tuneGrid = rf_grid_prost,
                trControl = control)

ggplot(rf_fit_prost, highlight = TRUE)
```

## Part D

```{r}
set.seed(1)
rf_prost <- randomForest(lpsa ~ ., data = dat_prostate,
                         mtry = 3)

rf_prost$importance
```

The variable for log cancer volume, at a value of `r rf_prost$importance[1]`, has the highest variable importance. The variable for log of prostate weight, at the value of `r rf_prost$importance[2]`, has the 2nd highest variable importance. 

### Caret for Boosting 

```{r}
gbm_grid <- expand.grid(n.trees = c(2000,3000),
                        interaction.depth = 2:10,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(1)
gbm_fit_prost <- train(lpsa ~ ., dat_prostate, 
                 method = "gbm",
                 tuneGrid = gbm_grid,
                 trControl = control,
                 verbose = FALSE)

ggplot(gbm_fit_prost, highlight = TRUE)
```

## Part E

```{r}
set.seed(1)
boosting_prost <- gbm(lpsa ~ ., data = dat_prostate,
                      distribution = "gaussian",
                      n.trees = 5000,
                      interaction.depth = 3,
                      shrinkage = 0.005,
                      cv.folds = 10)

nt <- gbm.perf(boosting_prost, method = "cv")

boosting_plot <- summary(boosting_prost, las = 2, cBars = 19, cex.names = 0.6)
```

The variable for log cancer volume, at a value of `r boosting_plot$rel.inf[1]`, has the highest variable importance. The variable for log of prostate weight, at the value of `r boosting_plot$rel.inf[2]`, has the 2nd highest variable importance.

## Part F

```{r}
resamp <- resamples(list(rf = rf_fit_prost, gbm = gbm_fit_prost))
summary(resamp)
```

I would pick the random forest model. When comparing the RMSE of the two models when resampling, random forest has the lower mean RMSE. 

# Problem 2

## Pulling & Creating Training/Test Datasets
```{r}
data("OJ")
dat_oj <- OJ %>% 
  janitor::clean_names()

set.seed(1)

rowTrain <- createDataPartition(y = dat_oj$purchase,
                               p = 799/1070,
                               list = FALSE)

train_dat_oj <- as.data.frame(dat_oj[rowTrain,])
test_dat_oj <- as.data.frame(dat_oj[-rowTrain,])

```

## Part A

```{r}
set.seed(1)
oj_tree <- rpart(purchase ~., data = train_dat_oj,
                 control = rpart.control(cp = 0))
#rpart.plot(oj_tree)

cp_table_oj <- printcp(oj_tree)
plotcp(oj_tree)
minErr_oj <- which.min(cp_table_oj[,4])

# minimum cross-validation error
oj_tree_2 <- prune(oj_tree, cp = cp_table_oj[minErr_oj, 1])
rpart.plot(oj_tree_2)

### Building Confusion Matrix
test_oj_tree_prob <- predict(oj_tree_2, newdata = test_dat_oj, type = "prob")
test_oj_tree_prob <- test_oj_tree_prob[,1]
test_oj_tree_pred <- rep("CH", 270)
test_oj_tree_pred[test_oj_tree_prob > 0.5] <- "MM"

oj_tree_matrix <- caret::confusionMatrix(data = as.factor(test_oj_tree_pred),
                reference = test_dat_oj$purchase,
                positive = "MM")
```

Based off the confusion matrix, it has an accuracy of `r oj_tree_matrix$overall[1]`. Thus, it has an error of `r 1 - oj_tree_matrix$overall[1]`. 

## Part B

```{r}
set.seed(1)
rf_oj <- ranger(purchase ~., train_dat_oj,
                mtry = 6,
                min.node.size = 5,
                splitrule = "gini",
                importance = "permutation",
                scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf_oj), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))
```

The customer brand loyalty for Citrus Hill orange juice, at a value of `r max(importance(rf_oj))`, has the highest variable importance. As you can see in the bar plot, it a much higher variable importance compared to the other variables. 

## Part C 

```{r}
set.seed(1)

boosting_oj <- gbm(purchase ~., data = train_dat_oj,
                   distribution = "gaussian",
                   n.trees = 5000,
                   interaction.depth = 3,
                   shrinkage = 0.005,
                   cv.folds = 10)

nt_oj = gbm.perf(boosting_oj, method = "cv")

summary(boosting_oj, las = 2, cBars = 19, cex.names = 0.6)
boosting_imp_oj <- summary(boosting_oj)

```

The customer brand loyalty for Citrus Hill orange juice, at a value of `r boosting_imp_oj$rel.inf[1]`, has the highest variable importance. As you can see in the bar plot, it a much higher variable importance compared to the other variables.The week of purchase variable, at a value of `r boosting_imp_oj$rel.inf[2]`, has the 2nd highest variable importance.